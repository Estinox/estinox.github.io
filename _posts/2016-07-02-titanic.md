---
layout: post
title: Kaggle Titanic Competition Walkthrough
tags:
    - python
    - notebook
--- 
# Kaggle Titanic Competition - Could Jack Have Lived? 
 
Welcome to my first, and rather long post on data analysis. Recently retook
Andrew Ng's [machine learning course](https://www.coursera.org/learn/machine-
learning) on Coursera, which I highly recommend as an intro course, and
[Harvard's CS109 Data Science](http://cs109.github.io/2015/) that's filled with
practical python examples and tutorials, so I thought I'd apply what I've
learned with some real-life data sets.

Kaggle's [Titanic competition](https://www.kaggle.com/c/titanic) is part of
their "getting started" competition for budding data scientists. The forum is
well populated with many sample solutions and pointers, so I'd thought I'd
whipping up a classifier and see how I fare on the Titanic journey. 
 
## Introduction and Conclusion (tl;dr)

Given the elaborative post, thought it'd be a good idea to post my thoughts at
the very top for the less patient.

The web version of this notebook might be a bit hard to digest, if you'd like to
try running each of the python cell blocks and to play around with the data,
click [here](https://github.com/Estinox/coding-
practices/blob/master/kaggle/titanic/titanic_kaggle.ipynb) for a copy.

I'll skip the introduction to what Titanis is about, as for the competition,
this notebook took me about 3 weekends to complete, given the limited number of
training size, so throwing away training rows wasn't optimal. Like any data
science project, I started by exloring every raw feature that was given, trying
to connect the high level relationships with what I know about ship wrecks. Once
I felt comfortable with the data set, a majority of my time was impute missing
values by cleaning and engineering new features. Without referring to external
resources, there's quite a bit of creativity that's needed to come up with
differentiating factors.

The way I wanted to construct this post is to give a narrative of my analysis by
starting with some introductory exploration then making incremental conclusions.
Everything is written in python with Jupyter notebook, so it's easy to clone my
repo and fiddle around with the data yourself, so let's get started! 
 
## Table of Contents
* [Data Libraries](#Load-Data-Set-and-Libraries)
* [Data Exploration](#Data-Exploration)
  * [Sex](#Sex)
  * [Passenger Class](#Passenger-Class)
  * [Port of Embarkment](#Port-of-Embarkment)
  * [Age](#Age)
  * [Fare](#Fare)
  * [Cabin](#Cabin)
* [Feature Engineering](#Feature-Engineering)
  * [Title](#Extract-Title-from-Name)
  * [Family Size](#Family-Size)
  * [Lone Traveller](#Lone-Travellers)
  * [Family Name](#Family-Name)
* [Impute Missing Values](#Impute-Missing-Values)
  * [Label Encoding](#Label-Encoding)
  * [Embarked](#Embarked)
  * [Ticket](#Ticket)
  * [Impute Age](#Impute-Age)
  * [Impute Fare](#Impute-Fare)
  * [Mother](#Mother)
  * [Cabin Class](#Cabin-Class)
  * [Cabin Number](#Cabin-Number)
  * [Cabin N-Tile](#Cabin-N-Tile)
* [Model Fitting](#Model-Fitting)
  * [Base Line](#Base-Line)
  * [Null Checks](#Null-Checks)
  * [Fitting](#Fitting)
* [Conclusion](#Conclusion) 
 
## Load Data Set and Libraries 

**In [1]:**

{% highlight python %}
import pandas as pd
import seaborn as sns
import matplotlib.pylab as plt
import numpy as np

import sklearn
from sklearn import preprocessing
from sklearn import ensemble
from sklearn import svm
from sklearn import linear_model
from sklearn import neighbors

%matplotlib inline
{% endhighlight %}
 
Read in our data set into train and test, then combine the two data frames to
form a union of features for future data imputation. 

**In [2]:**

{% highlight python %}
# Read into our datasets
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
test['Survived'] = np.NAN

union = pd.concat([train, test])
{% endhighlight %}
 
## Data Exploration

In this section, we'll explore some basic structure of our data and try to come
up creative ways of reformatting our features to make it more machine readable.

But first, let's famarize ourselves with the formatting of our data set. 

**In [3]:**

{% highlight python %}
union.describe()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Fare</th>
      <th>Parch</th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>SibSp</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1046.000000</td>
      <td>1308.000000</td>
      <td>1309.000000</td>
      <td>1309.000000</td>
      <td>1309.000000</td>
      <td>1309.000000</td>
      <td>891.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>29.881138</td>
      <td>33.295479</td>
      <td>0.385027</td>
      <td>655.000000</td>
      <td>2.294882</td>
      <td>0.498854</td>
      <td>0.383838</td>
    </tr>
    <tr>
      <th>std</th>
      <td>14.413493</td>
      <td>51.758668</td>
      <td>0.865560</td>
      <td>378.020061</td>
      <td>0.837836</td>
      <td>1.041658</td>
      <td>0.486592</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.170000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>21.000000</td>
      <td>7.895800</td>
      <td>0.000000</td>
      <td>328.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>28.000000</td>
      <td>14.454200</td>
      <td>0.000000</td>
      <td>655.000000</td>
      <td>3.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>39.000000</td>
      <td>31.275000</td>
      <td>0.000000</td>
      <td>982.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>80.000000</td>
      <td>512.329200</td>
      <td>9.000000</td>
      <td>1309.000000</td>
      <td>3.000000</td>
      <td>8.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>


 
### Sex
Now that you have a rough understanding of what each feature entail. Let's first
start by exploring how gender is related to the overall survival rate. 

**In [4]:**

{% highlight python %}
survived_filter = ~union.Survived.isnull()

survived_by_sex = union[survived_filter] [['Sex', 'Survived']].groupby('Sex').sum()

survived_by_sex['People'] = union[survived_filter].groupby('Sex').count().Survived

survived_by_sex['PctSurvived'] = survived_by_sex.Survived / survived_by_sex.People

survived_by_sex
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>People</th>
      <th>PctSurvived</th>
    </tr>
    <tr>
      <th>Sex</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>female</th>
      <td>233.0</td>
      <td>314</td>
      <td>0.742038</td>
    </tr>
    <tr>
      <th>male</th>
      <td>109.0</td>
      <td>577</td>
      <td>0.188908</td>
    </tr>
  </tbody>
</table>
</div>


 
This makes sense, Titanic's evacuation procedure had a women and children first
policy. So, vast majority of the women were led into a safety boat. 

**In [5]:**

{% highlight python %}
survived_by_sex[['Survived', 'People']].plot(kind='bar', rot=0)
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x117a3f668>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_13_1.png) 

 
### Passenger Class

What about survivial with respect to each passenger class (Pclass)? Do people
with higher social class tend live through the catastrophe? 

**In [6]:**

{% highlight python %}
survived_by_pclass = union[survived_filter][['Pclass', 'Survived']].groupby('Pclass').sum()

survived_by_pclass['People'] = union[survived_filter].groupby('Pclass').count().Survived

survived_by_pclass['PctSurvived'] = survived_by_pclass.Survived / survived_by_pclass.People

survived_by_pclass
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>People</th>
      <th>PctSurvived</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>136.0</td>
      <td>216</td>
      <td>0.629630</td>
    </tr>
    <tr>
      <th>2</th>
      <td>87.0</td>
      <td>184</td>
      <td>0.472826</td>
    </tr>
    <tr>
      <th>3</th>
      <td>119.0</td>
      <td>491</td>
      <td>0.242363</td>
    </tr>
  </tbody>
</table>
</div>


 
So people in higher pclasses do have higher survivability. It's hard to say why
without digging into the details, maybe it's the way they're dressed, the way
they conduct themselves during emergencies, better connected, or a higher
percentage of women (which we found from earlier exploration that females tend
to be led into life boats easier). All these questions are things we want to
keep in mind. 

**In [7]:**

{% highlight python %}
survived_by_pclass[['Survived', 'People']].plot(kind='bar', rot=0)
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x117b91390>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_17_1.png) 

 
How about if we separated social class and gender, how does our survival rate
look? 

**In [8]:**

{% highlight python %}
survived_by_pclass_sex = union[survived_filter].groupby(['Sex', 'Pclass']) \
    .apply(lambda x: x.Survived.sum() / len(x)) \
    .unstack()

survived_by_pclass_sex
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Pclass</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
    <tr>
      <th>Sex</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>female</th>
      <td>0.968085</td>
      <td>0.921053</td>
      <td>0.500000</td>
    </tr>
    <tr>
      <th>male</th>
      <td>0.368852</td>
      <td>0.157407</td>
      <td>0.135447</td>
    </tr>
  </tbody>
</table>
</div>


 
Looks between passenger classes, male in higher classes gets the biggest
survivability boost. And let's graph this for some visuals. 

**In [9]:**

{% highlight python %}
survived_by_pclass_sex.plot(kind='bar', title='Pct Survived by Pclass')
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x117ba59e8>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_21_1.png) 

 
### Port of Embarkment 

**In [10]:**

{% highlight python %}
survived_by_embarked = union[survived_filter][['Embarked', 'Survived']].groupby('Embarked').sum()

survived_by_embarked['People'] = union[survived_filter][['Embarked', 'Survived']].groupby('Embarked').count()

survived_by_embarked['PctSurvived'] = survived_by_embarked.Survived/survived_by_embarked.People

survived_by_embarked
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>People</th>
      <th>PctSurvived</th>
    </tr>
    <tr>
      <th>Embarked</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>C</th>
      <td>93.0</td>
      <td>168</td>
      <td>0.553571</td>
    </tr>
    <tr>
      <th>Q</th>
      <td>30.0</td>
      <td>77</td>
      <td>0.389610</td>
    </tr>
    <tr>
      <th>S</th>
      <td>217.0</td>
      <td>644</td>
      <td>0.336957</td>
    </tr>
  </tbody>
</table>
</div>


 
So embarking at port C gives a higher survivability rate, I wonder why. 

**In [11]:**

{% highlight python %}
survived_by_embarked[['Survived', 'People']].plot(kind='bar', rot=0)
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x117ae16d8>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_25_1.png) 

 
### Age 
 
So was the Titanic filled with kids? or seniors?

I've grouped people into buckets of 5 years, starting with age 0. 

**In [12]:**

{% highlight python %}
union.loc[~union.Age.isnull(), 'AgeGroup5'] = union.Age.dropna().apply(lambda x: int(x/5)*5)

ax = union.Age.hist(bins=len(union.AgeGroup5.unique()))
union[['Sex', 'AgeGroup5', 'Name']].groupby(['AgeGroup5', 'Sex']).count().unstack('Sex').plot(ax=ax)
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x117a6d630>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_28_1.png) 

 
We can get a bit more granular, let's break down the ages by passenger classes. 

**In [13]:**

{% highlight python %}
fig, axes = plt.subplots(1, 3, figsize=(16,4))

for i in range(3):
    union[union.Pclass == i+1]['Age'].hist(ax = axes[i])
    axes[i].set_title("Pclass {} Age Distribution".format(i+1))
{% endhighlight %}

 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_30_0.png) 

 
So we think that kids, like females, were given priority during the evacuation.
Let's check if this this true. 

**In [14]:**

{% highlight python %}
survived_by_agegroup5 = union[~union.Survived.isnull()][['AgeGroup5', 'Survived']].groupby('AgeGroup5').sum()

survived_by_agegroup5['People'] = union[~union.Survived.isnull()][['AgeGroup5', 'Survived']].groupby('AgeGroup5').count()

survived_by_agegroup5['PctSurvived'] = survived_by_agegroup5.Survived/survived_by_agegroup5.People

survived_by_agegroup5.head()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>People</th>
      <th>PctSurvived</th>
    </tr>
    <tr>
      <th>AgeGroup5</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0.0</th>
      <td>27.0</td>
      <td>40</td>
      <td>0.675000</td>
    </tr>
    <tr>
      <th>5.0</th>
      <td>11.0</td>
      <td>22</td>
      <td>0.500000</td>
    </tr>
    <tr>
      <th>10.0</th>
      <td>7.0</td>
      <td>16</td>
      <td>0.437500</td>
    </tr>
    <tr>
      <th>15.0</th>
      <td>34.0</td>
      <td>86</td>
      <td>0.395349</td>
    </tr>
    <tr>
      <th>20.0</th>
      <td>39.0</td>
      <td>114</td>
      <td>0.342105</td>
    </tr>
  </tbody>
</table>
</div>



**In [15]:**

{% highlight python %}
survived_by_agegroup5[['Survived', 'People']].plot(kind='bar', figsize=(16,4))
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11aca5748>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_33_1.png) 

 
#### Running survival rate 
 
Thought I'd try something different by plotting the cumulative survival rate
from age 0, to see how it drops off as you get older. 

**In [16]:**

{% highlight python %}
survived_by_age = union[~union.Survived.isnull()][train.Age <= 60][['Age', 'Survived']].sort_values('Age')

survived_by_age['CumSurvived'] = survived_by_age.Survived.cumsum()

survived_by_age['CumCount'] = [x+1 for x in range(len(survived_by_age))]

survived_by_age['CumSurvivalRate'] = survived_by_age.CumSurvived / survived_by_age.CumCount

survived_by_age.plot(kind='scatter', x='Age', y='CumSurvivalRate')
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11b109978>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_36_1.png) 


**In [17]:**

{% highlight python %}
survived_by_age[(survived_by_age.Age >= 5) & (survived_by_age.Age <= 8)].head()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Survived</th>
      <th>CumSurvived</th>
      <th>CumCount</th>
      <th>CumSurvivalRate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>233</th>
      <td>5.0</td>
      <td>1.0</td>
      <td>28.0</td>
      <td>41</td>
      <td>0.682927</td>
    </tr>
    <tr>
      <th>58</th>
      <td>5.0</td>
      <td>1.0</td>
      <td>29.0</td>
      <td>42</td>
      <td>0.690476</td>
    </tr>
    <tr>
      <th>777</th>
      <td>5.0</td>
      <td>1.0</td>
      <td>30.0</td>
      <td>43</td>
      <td>0.697674</td>
    </tr>
    <tr>
      <th>448</th>
      <td>5.0</td>
      <td>1.0</td>
      <td>31.0</td>
      <td>44</td>
      <td>0.704545</td>
    </tr>
    <tr>
      <th>751</th>
      <td>6.0</td>
      <td>1.0</td>
      <td>32.0</td>
      <td>45</td>
      <td>0.711111</td>
    </tr>
  </tbody>
</table>
</div>


 
### Fare 
 
Fare by default is like a proxy for passenger class. The more expensive the
fare, the wealthier you are.

Anyways, let's visual what the actual fare price distribution looks like. 

**In [18]:**

{% highlight python %}
union.Fare.hist(bins=100)
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11b324ac8>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_40_1.png) 


**In [19]:**

{% highlight python %}
union[['Pclass', 'Fare']].groupby('Pclass').describe().unstack()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">Fare</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>323.0</td>
      <td>87.508992</td>
      <td>80.447178</td>
      <td>0.0</td>
      <td>30.6958</td>
      <td>60.0000</td>
      <td>107.6625</td>
      <td>512.3292</td>
    </tr>
    <tr>
      <th>2</th>
      <td>277.0</td>
      <td>21.179196</td>
      <td>13.607122</td>
      <td>0.0</td>
      <td>13.0000</td>
      <td>15.0458</td>
      <td>26.0000</td>
      <td>73.5000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>708.0</td>
      <td>13.302889</td>
      <td>11.494358</td>
      <td>0.0</td>
      <td>7.7500</td>
      <td>8.0500</td>
      <td>15.2458</td>
      <td>69.5500</td>
    </tr>
  </tbody>
</table>
</div>


 
We need to come back and revisit Fare, since there are fares prices at $0 or
null. 

**In [20]:**

{% highlight python %}
fare_invalid_filter = (union.Fare.isnull()) | (union.Fare < 1)
len(union[fare_invalid_filter])
{% endhighlight %}




    18



**In [21]:**

{% highlight python %}
fig, ax = plt.subplots(figsize=(10,8))

sns.boxplot(data=union[~fare_invalid_filter], x='Pclass', y='Fare', ax=ax)
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11b722f98>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_44_1.png) 

 
Let's see if there's a relationshp between fare price and those who has survived 

**In [22]:**

{% highlight python %}
union[['Pclass', 'Fare']][(~fare_invalid_filter) & (union.Survived == 1)].groupby('Pclass').describe().unstack()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">Fare</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>136.0</td>
      <td>95.608029</td>
      <td>85.286820</td>
      <td>25.9292</td>
      <td>50.98545</td>
      <td>77.9583</td>
      <td>111.481225</td>
      <td>512.3292</td>
    </tr>
    <tr>
      <th>2</th>
      <td>87.0</td>
      <td>22.055700</td>
      <td>10.853502</td>
      <td>10.5000</td>
      <td>13.00000</td>
      <td>21.0000</td>
      <td>26.250000</td>
      <td>65.0000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>118.0</td>
      <td>13.810946</td>
      <td>10.663057</td>
      <td>6.9750</td>
      <td>7.77500</td>
      <td>8.5896</td>
      <td>15.887500</td>
      <td>56.4958</td>
    </tr>
  </tbody>
</table>
</div>



**In [23]:**

{% highlight python %}
union[['Pclass', 'Fare']][(~fare_invalid_filter) & (union.Survived == 0)].groupby('Pclass').describe().unstack()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">Fare</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>75.0</td>
      <td>68.996275</td>
      <td>60.224407</td>
      <td>5.0000</td>
      <td>29.7000</td>
      <td>50.00</td>
      <td>79.2000</td>
      <td>263.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>91.0</td>
      <td>20.692262</td>
      <td>14.938248</td>
      <td>10.5000</td>
      <td>12.9375</td>
      <td>13.00</td>
      <td>26.0000</td>
      <td>73.50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>369.0</td>
      <td>13.780497</td>
      <td>12.104365</td>
      <td>4.0125</td>
      <td>7.7500</td>
      <td>8.05</td>
      <td>15.2458</td>
      <td>69.55</td>
    </tr>
  </tbody>
</table>
</div>



**In [24]:**

{% highlight python %}
sns.boxplot(data=union[(~fare_invalid_filter) & (union.Survived == 1)], x='Pclass', y='Fare')
plt.title("Survivor's Fare Price by Pclass")

plt.figure()
sns.boxplot(data=union[(~fare_invalid_filter) & (union.Survived == 0)], x='Pclass', y='Fare')
plt.title("Non-Survivor's Fare Price by Pclass")
{% endhighlight %}




    <matplotlib.text.Text at 0x11bdfb860>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_48_1.png) 


 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_48_2.png) 

 
Percentage of people surivived by pclass

And not surpringly, people in better passenger classes had higher surivival rate 

**In [25]:**

{% highlight python %}
survived_by_fare_pclass = union[~union.Survived.isnull()][['Fare', 'Survived', 'Pclass']].groupby(['Pclass', 'Survived']).mean().unstack('Survived')
survived_by_fare_pclass.plot(kind='bar', rot=0)
plt.title('Survived by Pclass')
{% endhighlight %}




    <matplotlib.text.Text at 0x11beb5240>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_50_1.png) 

 
### Cabin 
 
There's a lot of missing values in the cabin feature, but let's check it out
anyways. We would think that the closer your cabin is to the life rafts, the
better your chances are.

First, we need to strip out the cabin number in the cabin column. 

**In [26]:**

{% highlight python %}
# Define cabin class
union.loc[~union.Cabin.isnull(), 'CabinClass'] = union.Cabin.dropna().str[0]
{% endhighlight %}

**In [27]:**

{% highlight python %}
cabinclass = union[~union.Survived.isnull()][['CabinClass', 'Survived']].groupby('CabinClass').sum()
cabinclass['People'] = union[~union.Survived.isnull()].groupby('CabinClass').count().Survived
cabinclass['PctSurvived'] = cabinclass.Survived / cabinclass.People
cabinclass['AvgFare'] = union[(~union.Survived.isnull()) & (~fare_invalid_filter)].groupby('CabinClass').mean().Fare
cabinclass['AvgAge'] = union[~union.Survived.isnull()].groupby('CabinClass').mean().Age
cabinclass['PctFemaleInCabin'] = union[~union.Survived.isnull()].groupby('CabinClass').apply(lambda x: len(x[x.Sex == 'female']) / len(x))

cabinclass
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>People</th>
      <th>PctSurvived</th>
      <th>AvgFare</th>
      <th>AvgAge</th>
      <th>PctFemaleInCabin</th>
    </tr>
    <tr>
      <th>CabinClass</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A</th>
      <td>7.0</td>
      <td>15</td>
      <td>0.466667</td>
      <td>42.454164</td>
      <td>44.833333</td>
      <td>0.066667</td>
    </tr>
    <tr>
      <th>B</th>
      <td>35.0</td>
      <td>47</td>
      <td>0.744681</td>
      <td>118.550464</td>
      <td>34.955556</td>
      <td>0.574468</td>
    </tr>
    <tr>
      <th>C</th>
      <td>35.0</td>
      <td>59</td>
      <td>0.593220</td>
      <td>100.151341</td>
      <td>36.086667</td>
      <td>0.457627</td>
    </tr>
    <tr>
      <th>D</th>
      <td>25.0</td>
      <td>33</td>
      <td>0.757576</td>
      <td>57.244576</td>
      <td>39.032258</td>
      <td>0.545455</td>
    </tr>
    <tr>
      <th>E</th>
      <td>24.0</td>
      <td>32</td>
      <td>0.750000</td>
      <td>46.026694</td>
      <td>38.116667</td>
      <td>0.468750</td>
    </tr>
    <tr>
      <th>F</th>
      <td>8.0</td>
      <td>13</td>
      <td>0.615385</td>
      <td>18.696792</td>
      <td>19.954545</td>
      <td>0.384615</td>
    </tr>
    <tr>
      <th>G</th>
      <td>2.0</td>
      <td>4</td>
      <td>0.500000</td>
      <td>13.581250</td>
      <td>14.750000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>T</th>
      <td>0.0</td>
      <td>1</td>
      <td>0.000000</td>
      <td>35.500000</td>
      <td>45.000000</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>


 
Weird how there's a single person in cabin T ...

More visualizations by cabin class! 

**In [28]:**

{% highlight python %}
cabinclass[['Survived', 'People']].plot(kind='bar', figsize=(14,5), title='Survived by Cabin Class')
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11bea8da0>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_56_1.png) 

 
## Feature Engineering 
 
Alright, the fun begins. Now we need to think of new features to add into our
classifier, what can we deduce from the raw components we have already examined
earlier?

One quick way of picking out the features with missing values is to look at the
non-null count from the describe() function. Also, we can visually inspect the
quantiles of each feature to see if it makes logically sense. For example, why
does the Fare feature has a min price of 0? 

**In [29]:**

{% highlight python %}
union.describe()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Fare</th>
      <th>Parch</th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>SibSp</th>
      <th>Survived</th>
      <th>AgeGroup5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1046.000000</td>
      <td>1308.000000</td>
      <td>1309.000000</td>
      <td>1309.000000</td>
      <td>1309.000000</td>
      <td>1309.000000</td>
      <td>891.000000</td>
      <td>1046.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>29.881138</td>
      <td>33.295479</td>
      <td>0.385027</td>
      <td>655.000000</td>
      <td>2.294882</td>
      <td>0.498854</td>
      <td>0.383838</td>
      <td>27.90631</td>
    </tr>
    <tr>
      <th>std</th>
      <td>14.413493</td>
      <td>51.758668</td>
      <td>0.865560</td>
      <td>378.020061</td>
      <td>0.837836</td>
      <td>1.041658</td>
      <td>0.486592</td>
      <td>14.60369</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.170000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>21.000000</td>
      <td>7.895800</td>
      <td>0.000000</td>
      <td>328.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>20.00000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>28.000000</td>
      <td>14.454200</td>
      <td>0.000000</td>
      <td>655.000000</td>
      <td>3.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>25.00000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>39.000000</td>
      <td>31.275000</td>
      <td>0.000000</td>
      <td>982.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>35.00000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>80.000000</td>
      <td>512.329200</td>
      <td>9.000000</td>
      <td>1309.000000</td>
      <td>3.000000</td>
      <td>8.000000</td>
      <td>1.000000</td>
      <td>80.00000</td>
    </tr>
  </tbody>
</table>
</div>


 
### Extracting Titles from Name

Name has a wealth of string information that we can take advantage of. Notice
how each person has a prefix, maybe we can extract this out and use it to
predict the age of the person if it's missing. 

**In [30]:**

{% highlight python %}
union['Title'] = union.Name.apply(lambda x: (x.split(',')[1]).split('.')[0][1:])
union[['Age','Title']].groupby('Title').describe().unstack()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">Age</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>Title</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Capt</th>
      <td>1.0</td>
      <td>70.000000</td>
      <td>NaN</td>
      <td>70.00</td>
      <td>70.00</td>
      <td>70.0</td>
      <td>70.00</td>
      <td>70.0</td>
    </tr>
    <tr>
      <th>Col</th>
      <td>4.0</td>
      <td>54.000000</td>
      <td>5.477226</td>
      <td>47.00</td>
      <td>51.50</td>
      <td>54.5</td>
      <td>57.00</td>
      <td>60.0</td>
    </tr>
    <tr>
      <th>Don</th>
      <td>1.0</td>
      <td>40.000000</td>
      <td>NaN</td>
      <td>40.00</td>
      <td>40.00</td>
      <td>40.0</td>
      <td>40.00</td>
      <td>40.0</td>
    </tr>
    <tr>
      <th>Dona</th>
      <td>1.0</td>
      <td>39.000000</td>
      <td>NaN</td>
      <td>39.00</td>
      <td>39.00</td>
      <td>39.0</td>
      <td>39.00</td>
      <td>39.0</td>
    </tr>
    <tr>
      <th>Dr</th>
      <td>7.0</td>
      <td>43.571429</td>
      <td>11.731115</td>
      <td>23.00</td>
      <td>38.00</td>
      <td>49.0</td>
      <td>51.50</td>
      <td>54.0</td>
    </tr>
    <tr>
      <th>Jonkheer</th>
      <td>1.0</td>
      <td>38.000000</td>
      <td>NaN</td>
      <td>38.00</td>
      <td>38.00</td>
      <td>38.0</td>
      <td>38.00</td>
      <td>38.0</td>
    </tr>
    <tr>
      <th>Lady</th>
      <td>1.0</td>
      <td>48.000000</td>
      <td>NaN</td>
      <td>48.00</td>
      <td>48.00</td>
      <td>48.0</td>
      <td>48.00</td>
      <td>48.0</td>
    </tr>
    <tr>
      <th>Major</th>
      <td>2.0</td>
      <td>48.500000</td>
      <td>4.949747</td>
      <td>45.00</td>
      <td>46.75</td>
      <td>48.5</td>
      <td>50.25</td>
      <td>52.0</td>
    </tr>
    <tr>
      <th>Master</th>
      <td>53.0</td>
      <td>5.482642</td>
      <td>4.161554</td>
      <td>0.33</td>
      <td>2.00</td>
      <td>4.0</td>
      <td>9.00</td>
      <td>14.5</td>
    </tr>
    <tr>
      <th>Miss</th>
      <td>210.0</td>
      <td>21.774238</td>
      <td>12.249077</td>
      <td>0.17</td>
      <td>15.00</td>
      <td>22.0</td>
      <td>30.00</td>
      <td>63.0</td>
    </tr>
    <tr>
      <th>Mlle</th>
      <td>2.0</td>
      <td>24.000000</td>
      <td>0.000000</td>
      <td>24.00</td>
      <td>24.00</td>
      <td>24.0</td>
      <td>24.00</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>Mme</th>
      <td>1.0</td>
      <td>24.000000</td>
      <td>NaN</td>
      <td>24.00</td>
      <td>24.00</td>
      <td>24.0</td>
      <td>24.00</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>Mr</th>
      <td>581.0</td>
      <td>32.252151</td>
      <td>12.422089</td>
      <td>11.00</td>
      <td>23.00</td>
      <td>29.0</td>
      <td>39.00</td>
      <td>80.0</td>
    </tr>
    <tr>
      <th>Mrs</th>
      <td>170.0</td>
      <td>36.994118</td>
      <td>12.901767</td>
      <td>14.00</td>
      <td>27.00</td>
      <td>35.5</td>
      <td>46.50</td>
      <td>76.0</td>
    </tr>
    <tr>
      <th>Ms</th>
      <td>1.0</td>
      <td>28.000000</td>
      <td>NaN</td>
      <td>28.00</td>
      <td>28.00</td>
      <td>28.0</td>
      <td>28.00</td>
      <td>28.0</td>
    </tr>
    <tr>
      <th>Rev</th>
      <td>8.0</td>
      <td>41.250000</td>
      <td>12.020815</td>
      <td>27.00</td>
      <td>29.50</td>
      <td>41.5</td>
      <td>51.75</td>
      <td>57.0</td>
    </tr>
    <tr>
      <th>Sir</th>
      <td>1.0</td>
      <td>49.000000</td>
      <td>NaN</td>
      <td>49.00</td>
      <td>49.00</td>
      <td>49.0</td>
      <td>49.00</td>
      <td>49.0</td>
    </tr>
    <tr>
      <th>the Countess</th>
      <td>1.0</td>
      <td>33.000000</td>
      <td>NaN</td>
      <td>33.00</td>
      <td>33.00</td>
      <td>33.0</td>
      <td>33.00</td>
      <td>33.0</td>
    </tr>
  </tbody>
</table>
</div>


 
Are some titles tend to dominate in one gender? 

**In [31]:**

{% highlight python %}
union[['Sex','Title','Survived']].groupby(['Title','Sex']).sum().unstack('Sex')
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">Survived</th>
    </tr>
    <tr>
      <th>Sex</th>
      <th>female</th>
      <th>male</th>
    </tr>
    <tr>
      <th>Title</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Capt</th>
      <td>NaN</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Col</th>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Don</th>
      <td>NaN</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Dona</th>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Dr</th>
      <td>1.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>Jonkheer</th>
      <td>NaN</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Lady</th>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Major</th>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Master</th>
      <td>NaN</td>
      <td>23.0</td>
    </tr>
    <tr>
      <th>Miss</th>
      <td>127.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Mlle</th>
      <td>2.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Mme</th>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Mr</th>
      <td>NaN</td>
      <td>81.0</td>
    </tr>
    <tr>
      <th>Mrs</th>
      <td>99.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Ms</th>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Rev</th>
      <td>NaN</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Sir</th>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>the Countess</th>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>


 
Working with all these different kinds of titles can overfit our data, let's try
classifying the less frequent titles into either Mr, Mrs, Miss, Master and
Officer. The goal is to come up with the fewest number of unique titles that
does not give us overlapping age ranges. 

**In [32]:**

{% highlight python %}
def title_mapping(x):
    #if x in set(['Capt', 'Col', 'Don', 'Major', 'Rev', 'Sir', 'Jonkheer']):
    if x.Title in set(['Don', 'Rev', 'Sir', 'Jonkheer']):
        return 'Mr'
    elif x.Title in set(['Lady', 'the Countess']):
        return 'Mrs'
    elif x.Title in set(['Mlle', 'Mme', 'Dona', 'Ms']):
        return "Miss"
    elif x.Title in set(['Major', 'Col', 'Capt']):
        return "Officer"
    elif x.Title == 'Dr' and x.Sex == 'female':
        return 'Mrs'
    elif x.Title == 'Dr' and x.Sex == 'male':
        return 'Mr'
    else:
        return x.Title

union.Title = union.apply(title_mapping, axis=1)
union.Title.unique()
{% endhighlight %}




    array(['Mr', 'Mrs', 'Miss', 'Master', 'Officer'], dtype=object)


 
Let see how well we have separated out the age groups by title. 

**In [33]:**

{% highlight python %}
union[['Age', 'Title']].groupby('Title').describe().unstack()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">Age</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>Title</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Master</th>
      <td>53.0</td>
      <td>5.482642</td>
      <td>4.161554</td>
      <td>0.33</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>9.0</td>
      <td>14.5</td>
    </tr>
    <tr>
      <th>Miss</th>
      <td>215.0</td>
      <td>21.914372</td>
      <td>12.171758</td>
      <td>0.17</td>
      <td>15.5</td>
      <td>22.0</td>
      <td>30.0</td>
      <td>63.0</td>
    </tr>
    <tr>
      <th>Mr</th>
      <td>598.0</td>
      <td>32.527592</td>
      <td>12.476329</td>
      <td>11.00</td>
      <td>23.0</td>
      <td>30.0</td>
      <td>40.0</td>
      <td>80.0</td>
    </tr>
    <tr>
      <th>Mrs</th>
      <td>173.0</td>
      <td>37.104046</td>
      <td>12.852049</td>
      <td>14.00</td>
      <td>27.0</td>
      <td>36.0</td>
      <td>47.0</td>
      <td>76.0</td>
    </tr>
    <tr>
      <th>Officer</th>
      <td>7.0</td>
      <td>54.714286</td>
      <td>8.440266</td>
      <td>45.00</td>
      <td>49.5</td>
      <td>53.0</td>
      <td>58.0</td>
      <td>70.0</td>
    </tr>
  </tbody>
</table>
</div>


 
### Family Size 

**In [34]:**

{% highlight python %}
union['FamilySize'] = union.Parch + union.SibSp + 1
union[['Parch', 'SibSp', 'FamilySize']][union.FamilySize > 3].head(3)
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Parch</th>
      <th>SibSp</th>
      <th>FamilySize</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7</th>
      <td>1</td>
      <td>3</td>
      <td>5</td>
    </tr>
    <tr>
      <th>13</th>
      <td>5</td>
      <td>1</td>
      <td>7</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1</td>
      <td>4</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>



**In [35]:**

{% highlight python %}
union.FamilySize.hist()
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11c347550>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_70_1.png) 


**In [36]:**

{% highlight python %}
def family_size_bucket(x):
    if x <= 1:
        return "Single"
    elif x <= 3:
        return "Small"
    elif x <= 5: 
        return "Median"
    else:
        return "Large"
    
union['FamilySizeBucket'] = union.FamilySize.apply(family_size_bucket)
{% endhighlight %}
 
### Lone Travellers 

**In [37]:**

{% highlight python %}
union['Alone'] = union.FamilySize.apply(lambda x: 1 if x == 1 else 0)
{% endhighlight %}

**In [38]:**

{% highlight python %}
survived_by_alone = union[~union.Survived.isnull()][['Survived', 'Alone']].groupby('Alone').sum()
survived_by_alone['People'] = union[~union.Survived.isnull()][['Survived', 'Alone']].groupby('Alone').count()
survived_by_alone['PctSurvived'] = survived_by_alone.Survived / survived_by_alone.People
survived_by_alone
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>People</th>
      <th>PctSurvived</th>
    </tr>
    <tr>
      <th>Alone</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>179.0</td>
      <td>354</td>
      <td>0.505650</td>
    </tr>
    <tr>
      <th>1</th>
      <td>163.0</td>
      <td>537</td>
      <td>0.303538</td>
    </tr>
  </tbody>
</table>
</div>


 
### Family Name 
 
Extract out the family names 

**In [39]:**

{% highlight python %}
union['FamilyName'] = union.Name.apply(lambda x: x.split(",")[0])
{% endhighlight %}

**In [40]:**

{% highlight python %}
survived_by_family_name = union[['Survived', 'FamilyName']].groupby('FamilyName').sum()
survived_by_family_name['FamilySize'] = union[['FamilySize', 'FamilyName']].groupby('FamilyName').max()

survived_by_family_name.head()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>FamilySize</th>
    </tr>
    <tr>
      <th>FamilyName</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Abbing</th>
      <td>0.0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Abbott</th>
      <td>1.0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>Abelseth</th>
      <td>NaN</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Abelson</th>
      <td>1.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Abrahamsson</th>
      <td>NaN</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>


 
Are families more likely to survive? 

**In [41]:**

{% highlight python %}
survived_by_family_size = survived_by_family_name.groupby('FamilySize').sum()
survived_by_family_size['FamilyCount'] = survived_by_family_name.groupby('FamilySize').count()
survived_by_family_size['PeopleInFamily'] = survived_by_family_size.FamilyCount * survived_by_family_size.index
survived_by_family_size['PctSurvived'] = survived_by_family_size.Survived / survived_by_family_size.PeopleInFamily

survived_by_family_size
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>FamilyCount</th>
      <th>PeopleInFamily</th>
      <th>PctSurvived</th>
    </tr>
    <tr>
      <th>FamilySize</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>153.0</td>
      <td>475</td>
      <td>475</td>
      <td>0.322105</td>
    </tr>
    <tr>
      <th>2</th>
      <td>89.0</td>
      <td>104</td>
      <td>208</td>
      <td>0.427885</td>
    </tr>
    <tr>
      <th>3</th>
      <td>65.0</td>
      <td>59</td>
      <td>177</td>
      <td>0.367232</td>
    </tr>
    <tr>
      <th>4</th>
      <td>21.0</td>
      <td>14</td>
      <td>56</td>
      <td>0.375000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4.0</td>
      <td>6</td>
      <td>30</td>
      <td>0.133333</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5.0</td>
      <td>5</td>
      <td>30</td>
      <td>0.166667</td>
    </tr>
    <tr>
      <th>7</th>
      <td>5.0</td>
      <td>2</td>
      <td>14</td>
      <td>0.357143</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.0</td>
      <td>1</td>
      <td>8</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.0</td>
      <td>1</td>
      <td>11</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>


 
## Impute Missing Values 
 
### Label Encoding

Categorical columns needs to be mapped to ints for some classifiers to work.
We'll use the sklearn.preprocessing.LableEncoder() to help us with this. 

**In [42]:**

{% highlight python %}
encoder_list = ['Sex', 'Pclass', 'Title','FamilyName', 'FamilySizeBucket']
encoder = []

for i, e in enumerate(encoder_list):
    print(i,e)
    encoder.append(sklearn.preprocessing.LabelEncoder())
    union['{}Encoding'.format(e)] = encoder[i].fit_transform(union[e])

{% endhighlight %}

    0 Sex
    1 Pclass
    2 Title
    3 FamilyName
    4 FamilySizeBucket

 
### Embarked 
 
This is a bit of an overkill, but we have 1 passenger missing her port of
emarkment. Simple analysis would've sufficed, but let's go the whole nine yards
anyways for completeness' sake. 

**In [43]:**

{% highlight python %}
embarked_features = ['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId',
       'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket',
       'CabinClass', 'Title', 'FamilySize', 'Alone', 'FamilyName']
{% endhighlight %}

**In [44]:**

{% highlight python %}
embarked_breakdown = union.loc[union.Pclass == 1, ['Pclass', 'Embarked', 'Survived']].groupby(['Pclass', 'Embarked']).sum()
embarked_breakdown['Count'] = union.loc[union.Pclass == 1, ['Pclass', 'Embarked', 'Survived']].groupby(['Pclass', 'Embarked']).count()
embarked_breakdown['AvgFare'] = union.loc[union.Pclass == 1, ['Pclass', 'Embarked', 'Fare']].groupby(['Pclass', 'Embarked']).mean().Fare
embarked_breakdown['AvgAge'] = union.loc[union.Pclass == 1, ['Pclass', 'Embarked', 'Age']].groupby(['Pclass', 'Embarked']).mean().Age

embarked_breakdown
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Survived</th>
      <th>Count</th>
      <th>AvgFare</th>
      <th>AvgAge</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th>Embarked</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3" valign="top">1</th>
      <th>C</th>
      <td>59.0</td>
      <td>85</td>
      <td>106.845330</td>
      <td>39.062500</td>
    </tr>
    <tr>
      <th>Q</th>
      <td>1.0</td>
      <td>2</td>
      <td>90.000000</td>
      <td>38.000000</td>
    </tr>
    <tr>
      <th>S</th>
      <td>74.0</td>
      <td>127</td>
      <td>72.148094</td>
      <td>39.121987</td>
    </tr>
  </tbody>
</table>
</div>



**In [45]:**

{% highlight python %}
union.loc[union.Pclass == 1, ['Pclass', 'Embarked', 'CabinClass', 'Survived']].groupby(['Pclass', 'Embarked', 'CabinClass']).count()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th></th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th>Embarked</th>
      <th>CabinClass</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="12" valign="top">1</th>
      <th rowspan="5" valign="top">C</th>
      <th>A</th>
      <td>7</td>
    </tr>
    <tr>
      <th>B</th>
      <td>22</td>
    </tr>
    <tr>
      <th>C</th>
      <td>21</td>
    </tr>
    <tr>
      <th>D</th>
      <td>11</td>
    </tr>
    <tr>
      <th>E</th>
      <td>5</td>
    </tr>
    <tr>
      <th>Q</th>
      <th>C</th>
      <td>2</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">S</th>
      <th>A</th>
      <td>8</td>
    </tr>
    <tr>
      <th>B</th>
      <td>23</td>
    </tr>
    <tr>
      <th>C</th>
      <td>36</td>
    </tr>
    <tr>
      <th>D</th>
      <td>18</td>
    </tr>
    <tr>
      <th>E</th>
      <td>20</td>
    </tr>
    <tr>
      <th>T</th>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



**In [46]:**

{% highlight python %}
union.loc[union.Pclass == 1, ['Pclass', 'Embarked', 'Sex', 'Survived']].groupby(['Pclass', 'Embarked', 'Sex']).count()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th></th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th>Embarked</th>
      <th>Sex</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="6" valign="top">1</th>
      <th rowspan="2" valign="top">C</th>
      <th>female</th>
      <td>43</td>
    </tr>
    <tr>
      <th>male</th>
      <td>42</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">Q</th>
      <th>female</th>
      <td>1</td>
    </tr>
    <tr>
      <th>male</th>
      <td>1</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">S</th>
      <th>female</th>
      <td>48</td>
    </tr>
    <tr>
      <th>male</th>
      <td>79</td>
    </tr>
  </tbody>
</table>
</div>


 
For imputation, let's train a k nearest neighbor classifer to figure out which
port of embarkment you're from. 

**In [47]:**

{% highlight python %}
embarked_features = ['Parch', 'PclassEncoding', 'SexEncoding', 'SibSp',
                    'TitleEncoding', 'FamilySize', 'Alone', 'FamilyNameEncoding', 'FamilySizeBucketEncoding']
{% endhighlight %}

**In [48]:**

{% highlight python %}
params = {'n_neighbors' : [5, 10, 15, 30, 50, 100, 200]}

clf_embarked = sklearn.grid_search.GridSearchCV(sklearn.neighbors.KNeighborsClassifier(), params, cv=5, n_jobs=4)
clf_embarked.fit(union[~union.Embarked.isnull()][embarked_features], union[~union.Embarked.isnull()].Embarked)

clf_embarked.predict(union[union.Embarked.isnull()][embarked_features])
{% endhighlight %}




    array(['S', 'S'], dtype=object)


 
KNN tells us that our passenger with no port of embarkment probably boarded from
"S", let's fill that in. 

**In [49]:**

{% highlight python %}
union.loc[union.Embarked.isnull(), 'Embarked'] = 'S'
{% endhighlight %}
 
Let's run our encoder code again on Embarked. 

**In [50]:**

{% highlight python %}
encoder_list = ['Embarked']
encoder = []

for i, e in enumerate(encoder_list):
    print(i,e)
    encoder.append(sklearn.preprocessing.LabelEncoder())
    union['{}Encoding'.format(e)] = encoder[i].fit_transform(union[e])

{% endhighlight %}

    0 Embarked

 
### Ticket 
 
There's a bit of judgemenet call in this one. The goal is to bucket the ticket
strings into various classes. After going through all the labels, you slowly
start to see some patterns. 

**In [51]:**

{% highlight python %}
def ticket_cleaning(x):
    ticket = x.split(' ')[0].replace('.', '').replace(r'/', '')
    if ticket.isdigit():
        return 'Integer_Group'
    elif 'TON' in ticket:
        return 'STON_Group'
    elif ticket[0] == 'A':
        return 'A_Group'
    elif ticket[0] == 'C':
        return 'C_Group'
    elif ticket[0] == 'F':
        return 'F_Group'
    elif ticket[0] == 'L':
        return 'L_Group'
    elif ticket[:2] == 'SC' or ticket == 'SOC':
        return 'SC_Group'
    elif ticket[:3] == 'SOP' or ticket == 'SP' or ticket == 'SWPP':
        return 'SP_Group'
    elif ticket[0] == 'P':
        return 'P_Group'
    elif ticket[0] == 'W':
        return 'W_Group'
    
    return ticket

union['TicketGroup'] = union['Ticket'].apply(ticket_cleaning)
union[['TicketGroup', 'Survived']].groupby('TicketGroup').count()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>TicketGroup</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A_Group</th>
      <td>29</td>
    </tr>
    <tr>
      <th>C_Group</th>
      <td>46</td>
    </tr>
    <tr>
      <th>F_Group</th>
      <td>7</td>
    </tr>
    <tr>
      <th>Integer_Group</th>
      <td>661</td>
    </tr>
    <tr>
      <th>L_Group</th>
      <td>4</td>
    </tr>
    <tr>
      <th>P_Group</th>
      <td>65</td>
    </tr>
    <tr>
      <th>SC_Group</th>
      <td>23</td>
    </tr>
    <tr>
      <th>SP_Group</th>
      <td>7</td>
    </tr>
    <tr>
      <th>STON_Group</th>
      <td>36</td>
    </tr>
    <tr>
      <th>W_Group</th>
      <td>13</td>
    </tr>
  </tbody>
</table>
</div>



**In [52]:**

{% highlight python %}
encoder_list = ['TicketGroup']
encoder = []

for i, e in enumerate(encoder_list):
    print(i,e)
    encoder.append(sklearn.preprocessing.LabelEncoder())
    union['{}Encoding'.format(e)] = encoder[i].fit_transform(union[e])

{% endhighlight %}

    0 TicketGroup

 
### Impute Age 
 
Yay, if you actually read this far. Age is a very important factor in predicting
survivability, so let's take in all the raw and engineered features and fill in
the blanks for our passengers. 

**In [53]:**

{% highlight python %}
age_features = ['EmbarkedEncoding', 'SexEncoding', 'PclassEncoding',
                'Alone', 'SibSp', 'Parch', 'FamilySize', 'TitleEncoding', 'TicketGroupEncoding']

#clf_age = sklearn.linear_model.RidgeCV(alphas=[0.1, 0.5, 1, 3, 10])
#clf_age = sklearn.linear_model.RidgeCV(alphas=[0.1, 0.5, 1, 3, 10])

params = {'n_neighbors' : [5, 10, 15, 30, 50, 100, 200]}

clf_age = sklearn.grid_search.GridSearchCV(sklearn.neighbors.KNeighborsRegressor(), params, cv=5, n_jobs=4)
clf_age.fit(union[~union.Age.isnull()][age_features], union[~union.Age.isnull()].Age)
print(clf_age.best_params_)
{% endhighlight %}

    {'n_neighbors': 15}


**In [54]:**

{% highlight python %}
pd.Series(clf_age.predict(union[union.Age.isnull()][age_features])).hist()
plt.title('Imputed Age')
{% endhighlight %}




    <matplotlib.text.Text at 0x11be70978>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_104_1.png) 

 
Now that we've trained our classifier, how does the new imputed age distribution
look like compared to the old one? 

**In [55]:**

{% highlight python %}
union['ImputedAge'] = union.Age
union.loc[(union.Age.isnull()), 'ImputedAge'] = clf_age.predict(union[(union.Age.isnull())][age_features])

union['ImputedAge'].hist()
union['Age'].hist()
plt.title('Imputed Age vs Original Age Distribution')
{% endhighlight %}




    <matplotlib.text.Text at 0x11bda60f0>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_106_1.png) 

 
### Impute Fare 

**In [56]:**

{% highlight python %}
union[['Pclass', 'Fare']].groupby('Pclass').describe().unstack()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="8" halign="left">Fare</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>323.0</td>
      <td>87.508992</td>
      <td>80.447178</td>
      <td>0.0</td>
      <td>30.6958</td>
      <td>60.0000</td>
      <td>107.6625</td>
      <td>512.3292</td>
    </tr>
    <tr>
      <th>2</th>
      <td>277.0</td>
      <td>21.179196</td>
      <td>13.607122</td>
      <td>0.0</td>
      <td>13.0000</td>
      <td>15.0458</td>
      <td>26.0000</td>
      <td>73.5000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>708.0</td>
      <td>13.302889</td>
      <td>11.494358</td>
      <td>0.0</td>
      <td>7.7500</td>
      <td>8.0500</td>
      <td>15.2458</td>
      <td>69.5500</td>
    </tr>
  </tbody>
</table>
</div>



**In [57]:**

{% highlight python %}
fig, axes = plt.subplots(1, 3, figsize=(16, 4))

#union[['Pclass', 'Fare']]
for i, pclass in enumerate(np.sort(union.Pclass.unique())):
    union[union.Pclass == pclass].Fare.hist(ax=axes[i])
    axes[i].set_title('Fare Distribution for Pclass={}'.format(pclass))
{% endhighlight %}

 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_109_0.png) 


**In [58]:**

{% highlight python %}
fare_features_knn = ['EmbarkedEncoding', 'SexEncoding', 'PclassEncoding',
                'Alone', 'SibSp', 'Parch', 'FamilySize', 'TitleEncoding',
                 'ImputedAge', 'TicketGroupEncoding']

fare_features_ridge = ['EmbarkedEncoding', 'PclassEncoding',
                'Alone', 'SibSp', 'Parch', 'ImputedAge']

fare_features = fare_features_knn

params = {'n_neighbors' : [2, 4, 7, 10, 15, 30, 50, 100, 200]}
clf_fare = sklearn.grid_search.GridSearchCV(sklearn.neighbors.KNeighborsRegressor(), params, cv=5, n_jobs=4)

#clf_fare = sklearn.linear_model.RidgeCV(alphas=[0.1, 0.5, 1, 3, 10])

clf_fare.fit(union[~union.Fare.isnull()][fare_features], union[~union.Fare.isnull()].Fare)
#print(clf_fare.best_params_)
{% endhighlight %}




    GridSearchCV(cv=5, error_score='raise',
           estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
              metric_params=None, n_jobs=1, n_neighbors=5, p=2,
              weights='uniform'),
           fit_params={}, iid=True, n_jobs=4,
           param_grid={'n_neighbors': [2, 4, 7, 10, 15, 30, 50, 100, 200]},
           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)



**In [59]:**

{% highlight python %}
union.loc[((union.Fare.isnull()) | (union.Fare <= 0)), ['Pclass', 'Fare']].groupby('Pclass').count()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Fare</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>



**In [60]:**

{% highlight python %}
union['ImputedFare'] = union.Fare

union.loc[union.Fare.isnull(), 'ImputedFare'] = clf_fare.predict(union[union.Fare.isnull()][fare_features])
{% endhighlight %}

**In [61]:**

{% highlight python %}
fig, axes = plt.subplots(1, 3, figsize=(16, 4))

for i, pclass in enumerate(np.sort(union.Pclass.unique())):
    null_fare_pclass_filter = ((union.Pclass == pclass) & ((union.Fare.isnull()) | (union.Fare <= 0)))
    pd.Series(clf_fare.predict(union[null_fare_pclass_filter][fare_features])).hist(ax=axes[i])
    axes[i].set_title('Imputed Fare for Pclass={}'.format(pclass))
{% endhighlight %}

 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_113_0.png) 


**In [62]:**

{% highlight python %}
union[union.Fare.isnull() | union.Fare <= 0][fare_features].head()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>EmbarkedEncoding</th>
      <th>SexEncoding</th>
      <th>PclassEncoding</th>
      <th>Alone</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>FamilySize</th>
      <th>TitleEncoding</th>
      <th>ImputedAge</th>
      <th>TicketGroupEncoding</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>179</th>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>36.0</td>
      <td>4</td>
    </tr>
    <tr>
      <th>263</th>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>40.0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>271</th>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>25.0</td>
      <td>4</td>
    </tr>
    <tr>
      <th>277</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>32.0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>302</th>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>19.0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>



**In [63]:**

{% highlight python %}
fig, axes = plt.subplots(1, 3, figsize=(16, 4))

for i, pclass in enumerate(np.sort(union.Pclass.unique())):
    union[union.Pclass == pclass].ImputedFare.hist(ax=axes[i])
    union[union.Pclass == pclass].Fare.hist(ax=axes[i])
    axes[i].set_title('Imputed Fare Distribution for Pclass={}'.format(pclass))
{% endhighlight %}

 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_115_0.png) 

 
### Mother 
 
"Mother and children first", so I think that's what they'd say as the Titanic is
sinking. Let's try to identify the mothers using the passenger's gender, age,
title, and whether if she has a child. 

**In [64]:**

{% highlight python %}
union['Mother'] = union.apply(lambda x: (x.Sex == 'female') & (x.ImputedAge >= 18) & (x.Parch > 0) & (x.Title == 'Mrs'), axis=1)
{% endhighlight %}
 
### Cabin Class 
 
Yikes, there's quite a bit of missing cabin values to impute. Not sure how our
classifier will do given the very limited number of training examples. Let's
give it a shot anyways. 

**In [65]:**

{% highlight python %}
union.Cabin.isnull().sum()
{% endhighlight %}




    1014


 
What are the different types of Cabin's given to us? 

**In [66]:**

{% highlight python %}
union.Cabin.unique()[:10]
{% endhighlight %}




    array([nan, 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6', 'C23 C25 C27',
           'B78'], dtype=object)



**In [67]:**

{% highlight python %}
union.loc[~union.Cabin.isnull(), 'CabinClass'] = union.Cabin[~union.Cabin.isnull()].apply(lambda x: x[0])
{% endhighlight %}

**In [68]:**

{% highlight python %}
encoder_list = ['CabinClass']
encoder = []

for i, e in enumerate(encoder_list):
    print(i,e)
    encoder.append(sklearn.preprocessing.LabelEncoder())
    union['{}Encoding'.format(e)] = union[e]
    union.loc[~union[e].isnull(), '{}Encoding'.format(e)] = encoder[i].fit_transform(union[~union[e].isnull()][e])

{% endhighlight %}

    0 CabinClass


**In [69]:**

{% highlight python %}
cabinclass_filter = ~union.CabinClass.isnull()
{% endhighlight %}
 
Probability of survival for each cabin class. 

**In [70]:**

{% highlight python %}
union[['CabinClass', 'Pclass', 'Survived']].groupby(['CabinClass', 'Pclass']).sum().unstack()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="3" halign="left">Survived</th>
    </tr>
    <tr>
      <th>Pclass</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
    <tr>
      <th>CabinClass</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A</th>
      <td>7.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>B</th>
      <td>35.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>C</th>
      <td>35.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>D</th>
      <td>22.0</td>
      <td>3.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>E</th>
      <td>18.0</td>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>F</th>
      <td>NaN</td>
      <td>7.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>G</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>T</th>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



**In [71]:**

{% highlight python %}
union.loc[(union.Pclass == 1) & (cabinclass_filter), 'CabinClassEncoding'].hist()
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11cd65b38>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_129_1.png) 


**In [72]:**

{% highlight python %}
fig, axes = plt.subplots(1, 3, figsize=(16,4))

cabinclass_filter = ~union.CabinClass.isnull()
for i, pclass in enumerate(np.sort(union.Pclass.unique())):
    union.loc[(union.Pclass == pclass) & (cabinclass_filter), 'CabinClassEncoding'].hist(ax=axes[i])
{% endhighlight %}

 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_130_0.png) 


**In [73]:**

{% highlight python %}
cabinclass_features = ['ImputedAge', 'EmbarkedEncoding', 'ImputedFare', 'Parch', 'PclassEncoding',
                       'SexEncoding', 'SibSp', 'TicketGroupEncoding']
{% endhighlight %}

**In [74]:**

{% highlight python %}
union['ImputedCabinClass'] = union.CabinClass
{% endhighlight %}

**In [75]:**

{% highlight python %}

params_cabinclass = {'n_estimators': [10, 20, 30]}

clf_cabinclass = sklearn.grid_search.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), params_cabinclass)
clf_cabinclass.fit(union[cabinclass_filter][cabinclass_features], union[cabinclass_filter].CabinClass)
{% endhighlight %}

    /Users/Estinox/bin/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
      % (min_labels, self.n_folds)), Warning)





    GridSearchCV(cv=None, error_score='raise',
           estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=None, max_features='auto', max_leaf_nodes=None,
                min_samples_leaf=1, min_samples_split=2,
                min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
                oob_score=False, random_state=None, verbose=0,
                warm_start=False),
           fit_params={}, iid=True, n_jobs=1,
           param_grid={'n_estimators': [10, 20, 30]}, pre_dispatch='2*n_jobs',
           refit=True, scoring=None, verbose=0)



**In [76]:**

{% highlight python %}
union.loc[~cabinclass_filter, 'ImputedCabinClass'] = clf_cabinclass.predict(union[~cabinclass_filter][cabinclass_features])
{% endhighlight %}

**In [77]:**

{% highlight python %}
encoder_list = ['ImputedCabinClass']
encoder = []

for i, e in enumerate(encoder_list):
    print(i,e)
    encoder.append(sklearn.preprocessing.LabelEncoder())
    union['{}Encoding'.format(e)] = union[e]
    union.loc[~union[e].isnull(), '{}Encoding'.format(e)] = encoder[i].fit_transform(union[~union[e].isnull()][e])

{% endhighlight %}

    0 ImputedCabinClass


**In [78]:**

{% highlight python %}
fig, axes = plt.subplots(1, 3, figsize=(16,4))

for i, pclass in enumerate(np.sort(union.Pclass.unique())):
    union.loc[(union.Pclass == pclass), 'ImputedCabinClassEncoding'].hist(ax=axes[i])
    union.loc[(union.Pclass == pclass), 'CabinClassEncoding'].hist(ax=axes[i])
{% endhighlight %}

 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_136_0.png) 

 
### Cabin Number 
 
Trying to be a creative here. My hypothesis is that the cabin numbers are laid
out either from the front to the back, so the number would indicate whether if
the room is at the front, middle, or the back of the ship. 

**In [79]:**

{% highlight python %}
def cabin_number_extract(x):
    if len(x) <= 1:
        return np.NAN
    
    for cabin in x.split(' '):
        if len(cabin) <= 1:
            continue
        else:
            return cabin[1:]
    return np.NAN

union.loc[~union.Cabin.isnull(), 'CabinNumber'] = union.Cabin[~union.Cabin.isnull()].apply(cabin_number_extract)
{% endhighlight %}

**In [80]:**

{% highlight python %}
union[cabinclass_filter][['CabinNumber', 'Cabin']].head()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CabinNumber</th>
      <th>Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>85</td>
      <td>C85</td>
    </tr>
    <tr>
      <th>3</th>
      <td>123</td>
      <td>C123</td>
    </tr>
    <tr>
      <th>6</th>
      <td>46</td>
      <td>E46</td>
    </tr>
    <tr>
      <th>10</th>
      <td>6</td>
      <td>G6</td>
    </tr>
    <tr>
      <th>11</th>
      <td>103</td>
      <td>C103</td>
    </tr>
  </tbody>
</table>
</div>



**In [81]:**

{% highlight python %}
features_cabinnumber = ['ImputedAge', 'ImputedCabinClassEncoding', 'EmbarkedEncoding', 'ImputedFare', 'Parch',
                        'PclassEncoding', 'SibSp', 'SexEncoding', 'FamilySize', 'Alone', 'TitleEncoding',
                        'FamilyNameEncoding', 'TicketGroupEncoding', 'Mother']
{% endhighlight %}

**In [82]:**

{% highlight python %}
encoder_list = ['CabinNumber']
encoder = []

for i, e in enumerate(encoder_list):
    print(i,e)
    encoder.append(sklearn.preprocessing.LabelEncoder())
    union['{}Encoding'.format(e)] = union[e]
    union.loc[~union[e].isnull(), '{}Encoding'.format(e)] = encoder[i].fit_transform(union[~union[e].isnull()][e])

{% endhighlight %}

    0 CabinNumber


**In [83]:**

{% highlight python %}
union['ImputedCabinNumber'] = union.CabinNumberEncoding
{% endhighlight %}

**In [84]:**

{% highlight python %}
params_cabinnumber = {'n_estimators': [10, 20, 50, 100]}
clf_cabinnumber = sklearn.grid_search.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), params_cabinnumber)

#clf_cabinnumber = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)

clf_cabinnumber.fit(union[~union.CabinNumber.isnull()][features_cabinnumber], union[~union.CabinNumber.isnull()].CabinNumberEncoding.astype(int))
{% endhighlight %}

    /Users/Estinox/bin/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
      % (min_labels, self.n_folds)), Warning)





    GridSearchCV(cv=None, error_score='raise',
           estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=None, max_features='auto', max_leaf_nodes=None,
                min_samples_leaf=1, min_samples_split=2,
                min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
                oob_score=False, random_state=None, verbose=0,
                warm_start=False),
           fit_params={}, iid=True, n_jobs=1,
           param_grid={'n_estimators': [10, 20, 50, 100]},
           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)



**In [85]:**

{% highlight python %}
union.loc[union.CabinNumber.isnull(), 'ImputedCabinNumber'] = clf_cabinnumber.predict(union[union.CabinNumber.isnull()][features_cabinnumber])
{% endhighlight %}

**In [86]:**

{% highlight python %}
encoder_list = ['ImputedCabinNumber']
encoder = []

for i, e in enumerate(encoder_list):
    print(i,e)
    encoder.append(sklearn.preprocessing.LabelEncoder())
    union['{}Encoding'.format(e)] = union[e]
    union.loc[~union[e].isnull(), '{}Encoding'.format(e)] = encoder[i].fit_transform(union[~union[e].isnull()][e])

{% endhighlight %}

    0 ImputedCabinNumber


**In [87]:**

{% highlight python %}
union[cabinclass_filter][['CabinNumber']].describe()
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CabinNumber</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>289</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>104</td>
    </tr>
    <tr>
      <th>top</th>
      <td>6</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div>



**In [88]:**

{% highlight python %}
fig, ax = plt.subplots(figsize=(16,4))

union.ImputedCabinNumberEncoding.hist(ax=ax)
union.CabinNumberEncoding.hist(ax=ax)
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11ab93a58>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_148_1.png) 

 
### Cabin N-Tile 
 
Take a look at the min and max of our cabins 

**In [89]:**

{% highlight python %}
union.loc[~union.CabinNumber.isnull(), 'CabinNumber'] = union[~union.CabinNumber.isnull()].CabinNumber.astype(int)
{% endhighlight %}

**In [90]:**

{% highlight python %}
union.CabinNumber.min(), union.CabinNumber.max()
{% endhighlight %}




    (2, 148)



**In [91]:**

{% highlight python %}
def cabin_tile(x):
    if x == np.NAN:
        return np.NAN

    if x <= 50:
        return 0
    elif x <= 100:
        return 1
    else:
        return 2

union['CabinTertile'] = union.CabinNumber
union.loc[~union.CabinTertile.isnull(), 'CabinTertile'] = union.CabinTertile[~union.CabinTertile.isnull()].apply(cabin_tile)
union['ImputedCabinTertile'] = union.CabinTertile
{% endhighlight %}

**In [92]:**

{% highlight python %}
params_cabinnumber = {'n_estimators': [10, 20, 50, 100]}
clf_cabinnumber = sklearn.grid_search.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), params_cabinnumber)

clf_cabinnumber = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)

clf_cabinnumber.fit(union[~union.CabinTertile.isnull()][features_cabinnumber], union[~union.CabinTertile.isnull()].CabinTertile.astype(int))
{% endhighlight %}




    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
               metric_params=None, n_jobs=1, n_neighbors=5, p=2,
               weights='uniform')



**In [93]:**

{% highlight python %}
union.loc[union.CabinTertile.isnull(), 'ImputedCabinTertile'] = clf_cabinnumber.predict(union[union.CabinTertile.isnull()][features_cabinnumber])
{% endhighlight %}

**In [94]:**

{% highlight python %}
fig, ax = plt.subplots(figsize=(16,4))

union.ImputedCabinTertile.hist(ax=ax)
union.CabinTertile.hist(ax=ax)
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x11cab7b38>



 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_156_1.png) 

 
## Model Fitting 
 
### Base Line

If we just predicted everyone to not surive, then we'd have 61.6% accuracy rate.
So this is the minimum base life we have to beat with our classifier. 

**In [95]:**

{% highlight python %}
len(union[union.Survived == 0]) / len(union[~union.Survived.isnull()])
{% endhighlight %}




    0.6161616161616161


 
### Null Checks 

**In [96]:**

{% highlight python %}
features = ['EmbarkedEncoding', 'Parch', 'PclassEncoding', 'SexEncoding', 'SibSp', 'TitleEncoding', 'FamilySize', 'Alone',
             'FamilyNameEncoding', 'ImputedAge', 'Mother', 'ImputedFare', 'TicketGroupEncoding', 'ImputedCabinTertile']
{% endhighlight %}
 
Check for null entries in our features 

**In [97]:**

{% highlight python %}
# Make sure that there are no Nan entries in our dataset
union[features][~union.Survived.isnull()][pd.isnull(union[features][~union.Survived.isnull()]).any(axis=1)]
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>EmbarkedEncoding</th>
      <th>Parch</th>
      <th>PclassEncoding</th>
      <th>SexEncoding</th>
      <th>SibSp</th>
      <th>TitleEncoding</th>
      <th>FamilySize</th>
      <th>Alone</th>
      <th>FamilyNameEncoding</th>
      <th>ImputedAge</th>
      <th>Mother</th>
      <th>ImputedFare</th>
      <th>TicketGroupEncoding</th>
      <th>ImputedCabinTertile</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
</div>


 
Okay, good, the above gave us no rows of null entries. We can continue. 
 
### Fitting

I've commented out a few classifiers I've tried to fit the model with. If you've
cloned this note book, you can easily uncomment out thos rows and check out the
results for yourselves.

For each classifier, I've decided to use grid search cross validation, and have
the grid search return the best parameter with the best cross validation score. 

**In [98]:**

{% highlight python %}
params_c = {'C':[0.005, 0.1,0.5,1,2,5]}
params_n_estimators = {'n_estimators':[100, 500,1000,2000,4000]}

clfs = []
training_filter = ~union.Survived.isnull()

#clfs.append(sklearn.grid_search.GridSearchCV(sklearn.linear_model.LogisticRegression(), params_c, cv=5, scoring='f1'))
#clfs.append(sklearn.grid_search.GridSearchCV(sklearn.svm.SVC(), params_c, cv=5, n_jobs=4))
clfs.append(sklearn.grid_search.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), params_n_estimators, cv=5, n_jobs=4))
#clfs.append(sklearn.grid_search.GridSearchCV(sklearn.ensemble.AdaBoostClassifier(), params_n_estimators, cv=5, n_jobs=4))
#clfs.append(sklearn.naive_bayes.GaussianNB())

for clf in clfs:
    clf.fit(union[training_filter][features], union[training_filter].Survived)

print('models fitted')
{% endhighlight %}

    models fitted


**In [99]:**

{% highlight python %}
for clf in clfs: 
    if isinstance(clf, sklearn.grid_search.GridSearchCV):
        print(clf.best_estimator_)
        print(clf.best_params_)
        print(clf.best_score_)
        print(clf.grid_scores_)
        print('\n')
    #elif isinstance(clf, sklearn.naive_bayes.GaussianNB):
    else:
        print(clf)
{% endhighlight %}

    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=None, max_features='auto', max_leaf_nodes=None,
                min_samples_leaf=1, min_samples_split=2,
                min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,
                oob_score=False, random_state=None, verbose=0,
                warm_start=False)
    {'n_estimators': 1000}
    0.837261503928
    [mean: 0.83277, std: 0.01839, params: {'n_estimators': 100}, mean: 0.83614, std: 0.02583, params: {'n_estimators': 500}, mean: 0.83726, std: 0.02555, params: {'n_estimators': 1000}, mean: 0.83502, std: 0.02497, params: {'n_estimators': 2000}, mean: 0.83502, std: 0.02683, params: {'n_estimators': 4000}]
    
    

 
How well have we trained against our own training set. 

**In [100]:**

{% highlight python %}
for clf in clfs:
    y_train_pred = clf.predict(union[training_filter][features])
    print(type(clf.best_estimator_) if isinstance(clf, sklearn.grid_search.GridSearchCV) else type(clf))
    print(sklearn.metrics.classification_report(y_train_pred, union[training_filter].Survived))

{% endhighlight %}

    <class 'sklearn.ensemble.forest.RandomForestClassifier'>
                 precision    recall  f1-score   support
    
            0.0       1.00      1.00      1.00       550
            1.0       1.00      1.00      1.00       341
    
    avg / total       1.00      1.00      1.00       891
    

 
How important were each of the features in our random forest? 

**In [101]:**

{% highlight python %}
for clf in clfs:
    if isinstance(clf, sklearn.grid_search.GridSearchCV):
        if isinstance(clf.best_estimator_, sklearn.ensemble.forest.RandomForestClassifier):
            print('Random Forest:')
            print(pd.DataFrame(list(zip(clf.best_estimator_.feature_importances_, features))
                               , columns=['Importance', 'Feature']).sort_values('Importance', ascending=False))
{% endhighlight %}

    Random Forest:
        Importance              Feature
    3     0.168709          SexEncoding
    11    0.161194          ImputedFare
    8     0.157862   FamilyNameEncoding
    9     0.143067           ImputedAge
    5     0.114327        TitleEncoding
    2     0.069930       PclassEncoding
    6     0.039600           FamilySize
    12    0.034325  TicketGroupEncoding
    4     0.027307                SibSp
    0     0.025159     EmbarkedEncoding
    13    0.024457  ImputedCabinTertile
    1     0.016596                Parch
    7     0.009526                Alone
    10    0.007940               Mother

 
How certain we are of our predictions? We can graph it out using predict proba. 

**In [102]:**

{% highlight python %}
for clf in clfs:
    if callable(getattr(clf, 'predict_proba', None)):
        pd.Series(clf.predict_proba(union[training_filter][features])[:,0]).hist()
        plt.title('Probability of Our Suvival Confidence')
{% endhighlight %}

 
![png]({{ site.url }}/assets/titanic_kaggle_files/titanic_kaggle_173_0.png) 

 
# Submission

Finally, we'll loop through all our classifiers and save the predicted csv file. 

**In [103]:**

{% highlight python %}
final_submission = []
for i, clf in enumerate(clfs):
    final_submission.append(union[~training_filter].copy())
    final_submission[i]['Survived'] = pd.DataFrame(clf.predict(union[~training_filter][features]))
    name = clf.best_estimator_.__class__.__name__ if isinstance(clf, sklearn.grid_search.GridSearchCV) else clf.__class__.__name__
    
    final_submission[i].Survived = final_submission[i].Survived.astype(int)
    final_submission[i].to_csv('titanic_{}.csv'.format(name), columns=['PassengerId', 'Survived'], index=False)
{% endhighlight %}
 
## Extras 
 
Over here, we have the learning curve analysis from Andrew Ng's machine learning
class. 

**In [104]:**

{% highlight python %}
from sklearn import learning_curve
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
    """
    Generate a simple plot of the test and traning learning curve.

    Parameters
    ----------
    estimator : object type that implements the "fit" and "predict" methods
        An object of that type which is cloned for each validation.

    title : string
        Title for the chart.

    X : array-like, shape (n_samples, n_features)
        Training vector, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape (n_samples) or (n_samples, n_features), optional
        Target relative to X for classification or regression;
        None for unsupervised learning.

    ylim : tuple, shape (ymin, ymax), optional
        Defines minimum and maximum yvalues plotted.

    cv : integer, cross-validation generator, optional
        If an integer is passed, it is the number of folds (defaults to 3).
        Specific cross-validation objects can be passed, see
        sklearn.cross_validation module for the list of possible objects

    n_jobs : integer, optional
        Number of jobs to run in parallel (default 1).
    """
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    return plt

#for clf in clfs:
#    name = clf.best_estimator_.__class__.__name__ if isinstance(clf, sklearn.grid_search.GridSearchCV) else clf.__class__.__name__
#    plot_learning_curve(clf, name, union[training_filter][features], union[training_filter].Survived, train_sizes=np.linspace(0.3,1,5))
{% endhighlight %}
 
# Conclusion

Was definitely fun developing this notebook, and hopefully it has helped anyone
who's trying to tackle the Kaggle Titanic problem. And again, if you're looking
for the source of the Jupyter notebook, it can be found in the introduction.
Thanks for reading! 
